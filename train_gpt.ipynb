{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-12T20:55:06.634460Z",
     "start_time": "2025-02-12T20:54:59.746022Z"
    }
   },
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import mlflow.pytorch\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710a6ac4275d6195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:38:45.849148Z",
     "start_time": "2025-01-23T10:38:45.750628Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./1_train_str.txt', 'r', encoding='utf-8') as file:\n",
    "    train_data = file.readlines()\n",
    "    \n",
    "with open('./1_eval_str.txt', 'r', encoding='utf-8') as file:\n",
    "    eval_data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b8c31e3e1715c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:38:47.064272Z",
     "start_time": "2025-01-23T10:38:47.055857Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(doc: str):\n",
    "    punc = string.punctuation\n",
    "    punc = punc.replace('|', '')\n",
    "    punc += '\\n\\r\\t'\n",
    "    return re.sub(' +', ' ', doc.translate(str.maketrans(punc, ' ' * len(punc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bb6910a0b1eb68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:39:08.690559Z",
     "start_time": "2025-01-23T10:39:06.625144Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_train_data = [remove_punctuation(doc) for doc in train_data]\n",
    "clean_eval_data = [remove_punctuation(doc) for doc in eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df57dc7682815af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:39:37.435124Z",
     "start_time": "2025-01-23T10:39:37.421868Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(clean_train_data, columns=['text'])\n",
    "eval_df = pd.DataFrame(clean_eval_data, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e22b64d32a9d321",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:39:56.453384Z",
     "start_time": "2025-01-23T10:39:56.322498Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1f0d2db436b5e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:39:59.277026Z",
     "start_time": "2025-01-23T10:39:57.507784Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78b5aa4443b823eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:40:22.508991Z",
     "start_time": "2025-01-23T10:40:22.505582Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=50)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # Utiliser input_ids comme labels\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "323fb94788c95d9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:40:42.959804Z",
     "start_time": "2025-01-23T10:40:22.867845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b3fef32ac14567aa9a2a67b7a32e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff07cf038d6e46c08e6ebf65c8bd3931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ef67fb4c4bc42cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:40:52.491894Z",
     "start_time": "2025-01-23T10:40:49.060573Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liu/.conda/envs/dac/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=20,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11575d4b2e5cd8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:44:20.390720Z",
     "start_time": "2025-01-23T10:42:18.710822Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mstart_run() \u001B[38;5;28;01mas\u001B[39;00m run:\n\u001B[0;32m----> 4\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;66;03m# Log des paramÃ¨tres et du modÃ¨le dans MLflow\u001B[39;00m\n\u001B[1;32m      6\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mlog_params({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: model_name, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m\"\u001B[39m: training_args\u001B[38;5;241m.\u001B[39mnum_train_epochs})\n",
      "File \u001B[0;32m~/.conda/envs/dac/lib/python3.11/site-packages/transformers/trainer.py:2052\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2050\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2051\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2052\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[1;32m   2053\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m   2054\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[1;32m   2055\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m   2056\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[1;32m   2057\u001B[0m     )\n",
      "File \u001B[0;32m~/.conda/envs/dac/lib/python3.11/site-packages/transformers/trainer.py:2153\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2150\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr_scheduler \u001B[38;5;241m=\u001B[39m deepspeed_init(\u001B[38;5;28mself\u001B[39m, num_training_steps\u001B[38;5;241m=\u001B[39mmax_steps)\n\u001B[1;32m   2152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m delay_optimizer_creation:\n\u001B[0;32m-> 2153\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_optimizer_and_scheduler(num_training_steps\u001B[38;5;241m=\u001B[39mmax_steps)\n\u001B[1;32m   2155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m TrainerState(\n\u001B[1;32m   2156\u001B[0m     stateful_callbacks\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m   2157\u001B[0m         cb \u001B[38;5;28;01mfor\u001B[39;00m cb \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mcallbacks \u001B[38;5;241m+\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(cb, ExportableState)\n\u001B[1;32m   2158\u001B[0m     ]\n\u001B[1;32m   2159\u001B[0m )\n\u001B[1;32m   2160\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mis_hyper_param_search \u001B[38;5;241m=\u001B[39m trial \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/dac/lib/python3.11/site-packages/transformers/trainer.py:1057\u001B[0m, in \u001B[0;36mTrainer.create_optimizer_and_scheduler\u001B[0;34m(self, num_training_steps)\u001B[0m\n\u001B[1;32m   1049\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_optimizer_and_scheduler\u001B[39m(\u001B[38;5;28mself\u001B[39m, num_training_steps: \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1051\u001B[0m \u001B[38;5;124;03m    Setup the optimizer and the learning rate scheduler.\u001B[39;00m\n\u001B[1;32m   1052\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1055\u001B[0m \u001B[38;5;124;03m    `create_scheduler`) in a subclass.\u001B[39;00m\n\u001B[1;32m   1056\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1057\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_optimizer()\n\u001B[1;32m   1058\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m IS_SAGEMAKER_MP_POST_1_10 \u001B[38;5;129;01mand\u001B[39;00m smp\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mcfg\u001B[38;5;241m.\u001B[39mfp16:\n\u001B[1;32m   1059\u001B[0m         \u001B[38;5;66;03m# If smp >= 1.10 and fp16 is enabled, we unwrap the optimizer\u001B[39;00m\n\u001B[1;32m   1060\u001B[0m         optimizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39moptimizer\n",
      "File \u001B[0;32m~/.conda/envs/dac/lib/python3.11/site-packages/transformers/trainer.py:1119\u001B[0m, in \u001B[0;36mTrainer.create_optimizer\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimizer_dict\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m optimizer_kwargs:\n\u001B[1;32m   1117\u001B[0m     optimizer_grouped_parameters \u001B[38;5;241m=\u001B[39m optimizer_kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimizer_dict\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;241m=\u001B[39m optimizer_cls(optimizer_grouped_parameters, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptimizer_kwargs)\n\u001B[1;32m   1121\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m optimizer_cls\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdam8bit\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1122\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/dac/lib/python3.11/site-packages/torch/optim/adamw.py:77\u001B[0m, in \u001B[0;36mAdamW.__init__\u001B[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001B[0m\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid weight_decay value: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mweight_decay\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     65\u001B[0m defaults \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m     66\u001B[0m     lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[1;32m     67\u001B[0m     betas\u001B[38;5;241m=\u001B[39mbetas,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     75\u001B[0m     fused\u001B[38;5;241m=\u001B[39mfused,\n\u001B[1;32m     76\u001B[0m )\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(params, defaults)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fused:\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m differentiable:\n",
      "File \u001B[0;32m~/.conda/envs/dac/lib/python3.11/site-packages/torch/optim/optimizer.py:371\u001B[0m, in \u001B[0;36mOptimizer.__init__\u001B[0;34m(self, params, defaults)\u001B[0m\n\u001B[1;32m    368\u001B[0m     param_groups \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m\"\u001B[39m: param_groups}]\n\u001B[1;32m    370\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m param_group \u001B[38;5;129;01min\u001B[39;00m param_groups:\n\u001B[0;32m--> 371\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_param_group(cast(\u001B[38;5;28mdict\u001B[39m, param_group))\n\u001B[1;32m    373\u001B[0m \u001B[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001B[39;00m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;66;03m# which I don't think exists\u001B[39;00m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001B[39;00m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_warned_capturable_if_run_uncaptured \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/dac/lib/python3.11/site-packages/torch/_compile.py:27\u001B[0m, in \u001B[0;36m_disable_dynamo.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     25\u001B[0m disable_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(fn, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__dynamo_disable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m disable_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 27\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\n\u001B[1;32m     29\u001B[0m     disable_fn \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mdisable(fn, recursive)\n\u001B[1;32m     30\u001B[0m     fn\u001B[38;5;241m.\u001B[39m__dynamo_disable \u001B[38;5;241m=\u001B[39m disable_fn\n",
      "File \u001B[0;32m~/.conda/envs/dac/lib/python3.11/site-packages/torch/_dynamo/__init__.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m convert_frame, eval_frame, resume_execution\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackends\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregistry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m list_backends, lookup_backend, register_backend\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001B[0;32m~/.conda/envs/dac/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:53\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_python_dispatch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     48\u001B[0m     _disable_current_modes,\n\u001B[1;32m     49\u001B[0m     is_in_torch_dispatch_mode,\n\u001B[1;32m     50\u001B[0m )\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_traceback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CapturedTraceback, format_traceback_short\n\u001B[0;32m---> 53\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config, exc, trace_rules\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbytecode_analysis\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m remove_dead_code, remove_pointless_jumps\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbytecode_transformation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     56\u001B[0m     check_inst_exn_tab_entries_valid,\n\u001B[1;32m     57\u001B[0m     Instruction,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     60\u001B[0m     transform_code_object,\n\u001B[1;32m     61\u001B[0m )\n",
      "File \u001B[0;32m~/.conda/envs/dac/lib/python3.11/site-packages/torch/_dynamo/trace_rules.py:46\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresume_execution\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TORCH_DYNAMO_RESUME_IN_PREFIX\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m getfile, hashable, NP_SUPPORTED_MODULES, unwrap_if_wrapper\n\u001B[0;32m---> 46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvariables\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     47\u001B[0m     BuiltinVariable,\n\u001B[1;32m     48\u001B[0m     FunctionalCallVariable,\n\u001B[1;32m     49\u001B[0m     FunctorchHigherOrderVariable,\n\u001B[1;32m     50\u001B[0m     NestedUserFunctionVariable,\n\u001B[1;32m     51\u001B[0m     PolyfilledFunctionVariable,\n\u001B[1;32m     52\u001B[0m     SkipFunctionVariable,\n\u001B[1;32m     53\u001B[0m     TorchInGraphFunctionVariable,\n\u001B[1;32m     54\u001B[0m     UserFunctionVariable,\n\u001B[1;32m     55\u001B[0m     UserMethodVariable,\n\u001B[1;32m     56\u001B[0m )\n\u001B[1;32m     59\u001B[0m np: Optional[types\u001B[38;5;241m.\u001B[39mModuleType] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/dac/lib/python3.11/site-packages/torch/_dynamo/variables/__init__.py:104\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msdpa\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SDPAParamsVariable\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensor\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     97\u001B[0m     FakeItemVariable,\n\u001B[1;32m     98\u001B[0m     NumpyNdarrayVariable,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    102\u001B[0m     UntypedStorageVariable,\n\u001B[1;32m    103\u001B[0m )\n\u001B[0;32m--> 104\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TorchCtxManagerClassVariable, TorchInGraphFunctionVariable\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01muser_defined\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m    106\u001B[0m     MutableMappingVariable,\n\u001B[1;32m    107\u001B[0m     RemovableHandleVariable,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    110\u001B[0m     WeakRefVariable,\n\u001B[1;32m    111\u001B[0m )\n\u001B[1;32m    114\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutogradFunctionContextVariable\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutogradFunctionVariable\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWithExitFunctionVariable\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    179\u001B[0m ]\n",
      "File \u001B[0;32m~/.conda/envs/dac/lib/python3.11/site-packages/torch/_dynamo/variables/torch.py:142\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;66;03m# Convert to dict for O(1) access times\u001B[39;00m\n\u001B[1;32m    133\u001B[0m constant_fold_functions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m\u001B[38;5;241m.\u001B[39mfromkeys(constant_fold_functions)\n\u001B[1;32m    136\u001B[0m tracing_state_functions \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    137\u001B[0m     torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_scripting: \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    138\u001B[0m     torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_tracing: \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    139\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_get_tracing_state: \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    140\u001B[0m     torch\u001B[38;5;241m.\u001B[39mfx\u001B[38;5;241m.\u001B[39m_symbolic_trace\u001B[38;5;241m.\u001B[39mis_fx_tracing: \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    141\u001B[0m     torch\u001B[38;5;241m.\u001B[39monnx\u001B[38;5;241m.\u001B[39mis_in_onnx_export: \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m--> 142\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mexternal_utils\u001B[38;5;241m.\u001B[39mis_compiling: \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    143\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39mis_compiling: \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    144\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcompiler\u001B[38;5;241m.\u001B[39mis_compiling: \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    145\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcompiler\u001B[38;5;241m.\u001B[39mis_dynamo_compiling: \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    146\u001B[0m     torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mmodules\u001B[38;5;241m.\u001B[39mactivation\u001B[38;5;241m.\u001B[39m_is_make_fx_tracing: \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    147\u001B[0m }\n\u001B[1;32m    149\u001B[0m bin_ops \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m\u001B[38;5;241m.\u001B[39mfromkeys([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madd\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msub\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmul\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqrt\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mBaseTorchVariable\u001B[39;00m(VariableTracker):\n",
      "\u001B[0;31mAttributeError\u001B[0m: partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    trainer.train()\n",
    "    # Log des paramÃ¨tres et du modÃ¨le dans MLflow\n",
    "    mlflow.log_params({\"model_name\": model_name, \"epochs\": training_args.num_train_epochs})\n",
    "    #mlflow.pytorch.log_model(model, artifact_path=\"model\", registered_model_name=\"OurModel\")\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "trainer.save_model('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5adf8598cb91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
